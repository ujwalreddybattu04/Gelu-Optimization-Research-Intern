{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649dd03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import EfficientNetForImageClassification\n",
    "\n",
    "# Standard GELU\n",
    "def gelu_np(x: np.ndarray) -> np.ndarray:\n",
    "    return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n",
    "\n",
    "# --- Your CachedGELU (float32 + slope based) ---\n",
    "class CachedGELU:\n",
    "    def __init__(self, x_min=-10.0, x_max=10.0, N=10001):\n",
    "        self.x_min = float(x_min)\n",
    "        self.x_max = float(x_max)\n",
    "        self.N = int(N)\n",
    "\n",
    "        self.step = (self.x_max - self.x_min) / (self.N - 1)\n",
    "        self.inv_step = 1.0 / self.step\n",
    "\n",
    "        x_table = np.linspace(self.x_min, self.x_max, self.N, dtype=np.float32)\n",
    "        self.y_table = 0.5 * x_table * (1.0 + erf(x_table / np.sqrt(2.0))).astype(np.float32)\n",
    "\n",
    "        # Precompute slopes for interpolation\n",
    "        self.slope = np.empty_like(self.y_table)\n",
    "        self.slope[:-1] = np.diff(self.y_table)\n",
    "        self.slope[-1] = self.slope[-2]\n",
    "\n",
    "    def apply(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xf = np.ascontiguousarray(X, dtype=np.float32)\n",
    "\n",
    "        # Compute table index and fractional part\n",
    "        idx_f = (Xf - self.x_min) * self.inv_step\n",
    "        idx = idx_f.astype(np.int32)\n",
    "        np.clip(idx, 0, self.N - 1, out=idx)\n",
    "        idx_f -= idx  # now idx_f is frac\n",
    "        out = self.y_table[idx] + self.slope[idx] * idx_f\n",
    "        mask_outside = (Xf < self.x_min) | (Xf > self.x_max)\n",
    "        if np.any(mask_outside):\n",
    "            X_out = Xf[mask_outside]\n",
    "            out[mask_outside] = 0.5 * X_out * (1.0 + erf(X_out / np.sqrt(2.0)))\n",
    "\n",
    "        return out\n",
    "\n",
    "# PyTorch Wrappers\n",
    "class PlainPythonGELU(nn.Module):\n",
    "    def __init__(self, torch_dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.np_dtype = np.float32 if torch_dtype == torch.float32 else np.float64\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_cpu = x.detach().cpu().numpy().astype(self.np_dtype)\n",
    "        out_np = gelu_np(x_cpu).astype(self.np_dtype)\n",
    "        return torch.from_numpy(out_np).to(x.device).type(x.dtype)\n",
    "\n",
    "class PlainPythonCachedGELU(nn.Module):\n",
    "    def __init__(self, x_min=-10, x_max=10, N=10001, torch_dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.np_dtype = np.float32\n",
    "        self.cache = CachedGELU(x_min=x_min, x_max=x_max, N=N)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_cpu = x.detach().cpu().numpy().astype(self.np_dtype)\n",
    "        out_np = self.cache.apply(x_cpu)\n",
    "        return torch.from_numpy(out_np).to(x.device).type(x.dtype)\n",
    "\n",
    "# Replace GELU/SiLU activations\n",
    "def replace_activations(module: nn.Module, new_act_module: nn.Module, inplace=False):\n",
    "    if not inplace:\n",
    "        module = copy.deepcopy(module)\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.SiLU) or isinstance(child, nn.GELU):\n",
    "            setattr(module, name, copy.deepcopy(new_act_module))\n",
    "        else:\n",
    "            replaced = replace_activations(child, new_act_module, inplace=True)\n",
    "            setattr(module, name, replaced)\n",
    "    return module\n",
    "\n",
    "# Benchmark inference time\n",
    "def benchmark_model(model: nn.Module, input_tensor: torch.Tensor, runs=20, warmup=2):\n",
    "    device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    inp = input_tensor.to(device).type(input_tensor.dtype)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            out_w = model(inp)\n",
    "            logits_w = out_w.logits if hasattr(out_w, 'logits') else out_w\n",
    "            _ = logits_w.detach().cpu()\n",
    "    times = []\n",
    "    last_output = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            t0 = time.perf_counter()\n",
    "            out = model(inp)\n",
    "            logits = out.logits if hasattr(out, 'logits') else out\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0) * 1000.0)\n",
    "            last_output = logits.detach().cpu()\n",
    "    return times, last_output\n",
    "\n",
    "# Main test script\n",
    "def main():\n",
    "    device = torch.device('cpu')\n",
    "    model_name = \"google/efficientnet-b7\"\n",
    "    print(\"Loading model:\", model_name)\n",
    "    model = EfficientNetForImageClassification.from_pretrained(model_name)\n",
    "    first_param = next(model.parameters())\n",
    "    model_dtype = first_param.dtype\n",
    "    print(\"Model parameter dtype:\", model_dtype)\n",
    "    input_size = model.config.image_size if hasattr(model.config, 'image_size') else 300\n",
    "    print(\"Using input size:\", input_size)\n",
    "\n",
    "    dummy_input = torch.randn(1, 3, input_size, input_size, dtype=model_dtype)\n",
    "\n",
    "    plain_act = PlainPythonGELU(torch_dtype=model_dtype)\n",
    "    cached_act = PlainPythonCachedGELU(x_min=-10, x_max=10, N=10001, torch_dtype=model_dtype)\n",
    "\n",
    "    model_plain = replace_activations(model, plain_act, inplace=False).to(model_dtype)\n",
    "    model_cached = replace_activations(model, cached_act, inplace=False).to(model_dtype)\n",
    "\n",
    "    runs = 20\n",
    "    warmup = 2\n",
    "\n",
    "    print(\"\\nBenchmarking on CPU only:\")\n",
    "    times_plain, out_plain = benchmark_model(model_plain, dummy_input, runs=runs, warmup=warmup)\n",
    "    mean_plain, std_plain = np.mean(times_plain), np.std(times_plain)\n",
    "    print(f\"Plain-Python GELU inference time: {mean_plain:.1f} ms ± {std_plain:.1f}\")\n",
    "\n",
    "    times_cached, out_cached = benchmark_model(model_cached, dummy_input, runs=runs, warmup=warmup)\n",
    "    mean_cached, std_cached = np.mean(times_cached), np.std(times_cached)\n",
    "    print(f\"Cached-Python GELU inference time: {mean_cached:.1f} ms ± {std_cached:.1f}\")\n",
    "\n",
    "    if out_plain.shape == out_cached.shape:\n",
    "        diff = out_plain.numpy() - out_cached.numpy()\n",
    "        max_abs = np.max(np.abs(diff))\n",
    "        mse = np.mean(diff**2)\n",
    "        print(f\"Output max abs difference: {max_abs:.6f}\")\n",
    "        print(f\"Output MSE: {mse:.6e}\")\n",
    "    else:\n",
    "        print(\"Output shapes differ; cannot compare.\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
